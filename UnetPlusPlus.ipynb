{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10929263,"sourceType":"datasetVersion","datasetId":6795467,"isSourceIdPinned":true},{"sourceId":10934538,"sourceType":"datasetVersion","datasetId":6799526},{"sourceId":276840,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":237070,"modelId":258759}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install segmentation-models-pytorch\n!pip install lightning albumentations","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:27:21.816547Z","iopub.execute_input":"2025-03-06T07:27:21.816853Z","iopub.status.idle":"2025-03-06T07:27:28.832874Z","shell.execute_reply.started":"2025-03-06T07:27:21.816827Z","shell.execute_reply":"2025-03-06T07:27:28.831823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 1 : Import des biblioth√®ques n√©cessaires\nimport os\nimport glob\nimport json\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\n# Vous pouvez ajouter d'autres imports d'albumentations si besoin\n\nimport segmentation_models_pytorch as smp\n\n# V√©rifier si un GPU est disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Utilisation de l'appareil :\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:07:30.146762Z","iopub.execute_input":"2025-03-06T07:07:30.147102Z","iopub.status.idle":"2025-03-06T07:07:40.975832Z","shell.execute_reply.started":"2025-03-06T07:07:30.147075Z","shell.execute_reply":"2025-03-06T07:07:40.975100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] : Classe dataset adapt√©e sans fichier JSON\nclass Sentinel2Dataset(Dataset):\n    \"\"\"\n    Dataset pour la segmentation des routes.\n    Il extrait des patchs de taille patch_size x patch_size √† partir d'images de grande taille\n    en utilisant une fen√™tre glissante sans chevauchement.\n    \"\"\"\n    def __init__(self, root_dir, split='train', transform=None, patch_size=512, stride=512):\n        \"\"\"\n        Args:\n            root_dir (str): Chemin racine du dataset (ex: \"/kaggle/input/mon_dataset\").\n            split (str): 'train', 'valid' ou 'test'.\n            transform: Transformations √† appliquer (albumentations).\n            patch_size (int): Taille du patch (par d√©faut 512).\n            stride (int): Pas de la fen√™tre glissante (par d√©faut 512, donc sans chevauchement).\n        \"\"\"\n        self.transform = transform\n        self.patch_size = patch_size\n        self.stride = stride\n        \n        # D√©finir les r√©pertoires d'images et de masques en fonction du split\n        self.image_dir = os.path.join(root_dir, split, \"images\")\n        self.mask_dir = os.path.join(root_dir, split, \"masks\")\n        \n        # R√©cup√©rer la liste des fichiers d'images et masques\n        self.image_files = sorted(glob.glob(os.path.join(self.image_dir, \"*\")))\n        self.mask_files = sorted(glob.glob(os.path.join(self.mask_dir, \"*\")))\n        \n        # V√©rifier que le nombre d'images et de masques correspond\n        assert len(self.image_files) == len(self.mask_files), \"Le nombre d'images et de masques ne correspond pas!\"\n        \n        # Cr√©er la liste des patchs : pour chaque image, on stocke (chemin_image, chemin_masque, x, y)\n        self.patches = []\n        for img_path, mask_path in zip(self.image_files, self.mask_files):\n            img = cv2.imread(img_path)\n            if img is None:\n                continue\n            h, w, _ = img.shape\n            for y in range(0, h, self.stride):\n                for x in range(0, w, self.stride):\n                    if y + self.patch_size <= h and x + self.patch_size <= w:\n                        self.patches.append((img_path, mask_path, x, y))\n                        \n        print(f\"Nombre de patchs pour le split {split} : {len(self.patches)}\")\n    \n    def __len__(self):\n        return len(self.patches)\n    \n    def __getitem__(self, index):\n        img_path, mask_path, x, y = self.patches[index]\n        # Lire l'image et le masque\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    \n        # Extraire le patch correspondant\n        image_patch = image[y:y+self.patch_size, x:x+self.patch_size]\n        mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]\n    \n        # Normaliser le masque (convertir 0/255 en 0/1)\n        mask_patch = mask_patch.astype(np.float32) / 255.0\n    \n        # Appliquer les transformations (si d√©finies, par exemple pour l'entra√Ænement)\n        if self.transform:\n           augmented = self.transform(image=image_patch, mask=mask_patch)\n           image_patch = augmented['image']\n           mask_patch = augmented['mask']\n    \n        # Normaliser l'image de [0, 255] √† [0, 1]\n        image_patch = image_patch.astype(np.float32) / 255.0\n       # Changer l'ordre des canaux pour obtenir (C, H, W)\n        image_patch = np.transpose(image_patch, (2, 0, 1))\n        image_patch = torch.tensor(image_patch, dtype=torch.float)\n    \n        # Convertir le masque en tensor (la dimension du canal sera ajout√©e pour correspondre √† la sortie du mod√®le)\n        mask_patch = torch.tensor(mask_patch, dtype=torch.float)\n        mask_patch = mask_patch.unsqueeze(0)\n    \n        return image_patch, mask_patch\n\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:08:08.505728Z","iopub.execute_input":"2025-03-06T07:08:08.506074Z","iopub.status.idle":"2025-03-06T07:08:08.516862Z","shell.execute_reply.started":"2025-03-06T07:08:08.506049Z","shell.execute_reply":"2025-03-06T07:08:08.515996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 3 : D√©finition de la fonction d'augmentation\ndef transformer(p=0.5):\n    \"\"\"\n    Fonction de transformation utilisant albumentations pour la data augmentation.\n    Seules les images d'entra√Ænement seront augment√©es.\n    \"\"\"\n    return A.Compose([\n        # Recadrage al√©atoire redimensionn√©\n        A.RandomResizedCrop(\n            height=512,\n            width=512,\n            p=0.2\n        ),\n        # D√©placement, mise √† l'√©chelle et rotation\n        A.ShiftScaleRotate(\n            p=0.5,\n            shift_limit=0.0625,\n            scale_limit=0.05,\n            rotate_limit=10,\n            border_mode=0,  # bord constant\n            value=0,\n            mask_value=0,\n            interpolation=2,  # interpolation bicubique\n        ),\n        # Flip, transposition et rotation de 90 degr√©s al√©atoire\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        A.RandomRotate90(p=0.5),\n    ], p=p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:08:14.216524Z","iopub.execute_input":"2025-03-06T07:08:14.216810Z","iopub.status.idle":"2025-03-06T07:08:14.222221Z","shell.execute_reply.started":"2025-03-06T07:08:14.216789Z","shell.execute_reply":"2025-03-06T07:08:14.220958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] : Cr√©ation des datasets et DataLoaders sans fichier JSON\n# D√©finir le chemin racine de votre dataset Kaggle\ndataset_root = \"/kaggle/input/deepglobe/deepglobe\"  # Adaptez ce chemin selon votre environnement\n\n# Cr√©ation des datasets pour l'entra√Ænement et la validation\ntrain_dataset = Sentinel2Dataset(root_dir=dataset_root, split='train', transform=transformer(p=0.5))\nvalid_dataset = Sentinel2Dataset(root_dir=dataset_root, split='valid', transform=None)\n\n# Cr√©ation des DataLoaders (batch = 8)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:08:19.695031Z","iopub.execute_input":"2025-03-06T07:08:19.695341Z","iopub.status.idle":"2025-03-06T07:10:57.602242Z","shell.execute_reply.started":"2025-03-06T07:08:19.695316Z","shell.execute_reply":"2025-03-06T07:10:57.601377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 5 : D√©finition du mod√®le U-Net avec ResNet50\n# Cr√©ation du mod√®le avec segmentation_models_pytorch\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",        # Choix du backbone\n    encoder_weights=\"imagenet\",       # Poids pr√©-entra√Æn√©s sur ImageNet\n    in_channels=3,                    # Entr√©e en RGB\n    classes=1,                        # Segmentation binaire (1 canal en sortie)\n)\n\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:35:20.925947Z","iopub.execute_input":"2025-03-05T22:35:20.926274Z","iopub.status.idle":"2025-03-05T22:35:22.539906Z","shell.execute_reply.started":"2025-03-05T22:35:20.926249Z","shell.execute_reply":"2025-03-05T22:35:22.539072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 6 : D√©finition de la fonction de perte et de l'optimiseur\ncriterion = nn.BCEWithLogitsLoss()  # Adapt√© pour la segmentation binaire\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:35:30.766138Z","iopub.execute_input":"2025-03-05T22:35:30.766425Z","iopub.status.idle":"2025-03-05T22:35:30.771444Z","shell.execute_reply.started":"2025-03-05T22:35:30.766405Z","shell.execute_reply":"2025-03-05T22:35:30.770613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 7 : Fonctions de calcul des m√©triques\ndef compute_metrics(outputs, masks, threshold=0.5):\n    \"\"\"\n    Calcule les vrais positifs (TP), faux positifs (FP) et faux n√©gatifs (FN) pour un batch.\n    \n    Args:\n        outputs (tensor): sorties brutes du mod√®le (logits) de taille (B, 1, H, W)\n        masks (tensor): masques ground truth de taille (B, 1, H, W)\n        threshold (float): seuil pour binariser la sortie (apr√®s sigmoid)\n        \n    Returns:\n        TP, FP, FN cumul√©s pour le batch.\n    \"\"\"\n    # Appliquer la sigmoid pour obtenir les probabilit√©s\n    probs = torch.sigmoid(outputs)\n    preds = (probs > threshold).float()\n    \n    masks = masks.float()\n    TP = (preds * masks).sum()\n    FP = (preds * (1 - masks)).sum()\n    FN = ((1 - preds) * masks).sum()\n    \n    return TP.item(), FP.item(), FN.item()\n\ndef calculate_epoch_metrics(TP, FP, FN, eps=1e-6):\n    \"\"\"\n    Calcule IoU, Pr√©cision, Rappel et F1-score √† partir des sommes de TP, FP et FN.\n    \"\"\"\n    IoU = TP / (TP + FP + FN + eps)\n    precision = TP / (TP + FP + eps)\n    recall = TP / (TP + FN + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    return IoU, precision, recall, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:35:33.987826Z","iopub.execute_input":"2025-03-05T22:35:33.988138Z","iopub.status.idle":"2025-03-05T22:35:33.993596Z","shell.execute_reply.started":"2025-03-05T22:35:33.988113Z","shell.execute_reply":"2025-03-05T22:35:33.992734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 8 : Boucle d'entra√Ænement et validation avec enregistrement des m√©triques\nnum_epochs = 8  # Vous pouvez ajuster le nombre d'√©poques\nmetrics_history = []   # Liste pour stocker les m√©triques par √©poque\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    train_TP, train_FP, train_FN = 0, 0, 0\n    \n    for images, masks in train_loader:\n        images = images.to(device)\n        masks = masks.to(device).float()  # Conversion en float pour la loss\n        \n        optimizer.zero_grad()\n        outputs = model(images)  # Sortie de taille (B, 1, H, W)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n        TP, FP, FN = compute_metrics(outputs, masks)\n        train_TP += TP\n        train_FP += FP\n        train_FN += FN\n        \n    train_loss /= len(train_loader.dataset)\n    train_IoU, train_precision, train_recall, train_f1 = calculate_epoch_metrics(train_TP, train_FP, train_FN)\n    \n    # Phase de validation\n    model.eval()\n    valid_loss = 0.0\n    valid_TP, valid_FP, valid_FN = 0, 0, 0\n    with torch.no_grad():\n        for images, masks in valid_loader:\n            images = images.to(device)\n            masks = masks.to(device).float()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            valid_loss += loss.item() * images.size(0)\n            TP, FP, FN = compute_metrics(outputs, masks)\n            valid_TP += TP\n            valid_FP += FP\n            valid_FN += FN\n            \n    valid_loss /= len(valid_loader.dataset)\n    valid_IoU, valid_precision, valid_recall, valid_f1 = calculate_epoch_metrics(valid_TP, valid_FP, valid_FN)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {valid_loss:.4f}\")\n    print(f\"Train - IoU: {train_IoU:.4f}, Pr√©cision: {train_precision:.4f}, Rappel: {train_recall:.4f}, F1: {train_f1:.4f}\")\n    print(f\"Valid - IoU: {valid_IoU:.4f}, Pr√©cision: {valid_precision:.4f}, Rappel: {valid_recall:.4f}, F1: {valid_f1:.4f}\\n\")\n    \n    # Enregistrer les m√©triques de l'√©poque\n    metrics_history.append({\n         'epoch': epoch+1,\n         'train_loss': train_loss,\n         'valid_loss': valid_loss,\n         'train_IoU': train_IoU,\n         'train_precision': train_precision,\n         'train_recall': train_recall,\n         'train_f1': train_f1,\n         'valid_IoU': valid_IoU,\n         'valid_precision': valid_precision,\n         'valid_recall': valid_recall,\n         'valid_f1': valid_f1,\n    })\n\n# Sauvegarder l'historique des m√©triques dans un fichier CSV\nmetrics_df = pd.DataFrame(metrics_history)\nmetrics_df.to_csv(\"metrics_history111.csv\", index=False)\nprint(\"Les m√©triques ont √©t√© sauvegard√©es dans 'metrics_history111.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:35:37.772166Z","iopub.execute_input":"2025-03-05T22:35:37.772458Z","iopub.status.idle":"2025-03-06T04:59:08.130056Z","shell.execute_reply.started":"2025-03-05T22:35:37.772437Z","shell.execute_reply":"2025-03-06T04:59:08.129196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 9 : Affichage des courbes d'√©volution des m√©triques\nepochs = metrics_df['epoch']\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(epochs, metrics_df['train_IoU'], label='Train IoU')\nplt.plot(epochs, metrics_df['valid_IoU'], label='Valid IoU')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"IoU\")\nplt.legend()\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs, metrics_df['train_precision'], label='Train Pr√©cision')\nplt.plot(epochs, metrics_df['valid_precision'], label='Valid Pr√©cision')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Pr√©cision\")\nplt.legend()\n\nplt.subplot(2, 2, 3)\nplt.plot(epochs, metrics_df['train_recall'], label='Train Rappel')\nplt.plot(epochs, metrics_df['valid_recall'], label='Valid Rappel')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Rappel\")\nplt.legend()\n\nplt.subplot(2, 2, 4)\nplt.plot(epochs, metrics_df['train_f1'], label='Train F1-score')\nplt.plot(epochs, metrics_df['valid_f1'], label='Valid F1-score')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"F1-score\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T05:53:27.566118Z","iopub.execute_input":"2025-03-06T05:53:27.566398Z","iopub.status.idle":"2025-03-06T05:53:27.639957Z","shell.execute_reply.started":"2025-03-06T05:53:27.566375Z","shell.execute_reply":"2025-03-06T05:53:27.638904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 9 : Sauvegarde du mod√®le entra√Æn√©\n# Sauvegarder le mod√®le apr√®s l'entra√Ænement\ntorch.save(model.state_dict(), 'road_segmentation_model.pth')\nprint(\"Mod√®le sauvegard√© avec succ√®s.\")\n\n# %% [code] Cellule 10 : Fonction d'inf√©rence pour les grandes images\ndef predict_large_image(image_path, model, patch_size=512, device=device):\n    \"\"\"\n    Pr√©dit le masque de segmentation pour une grande image sans redimensionnement.\n    D√©coupe l'image en patches, effectue la pr√©diction sur chaque patch et recompose le masque final.\n    \"\"\"\n    # Charger l'image\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original_h, original_w, _ = image.shape\n    \n    # Appliquer un padding si n√©cessaire\n    pad_h = (patch_size - original_h % patch_size) % patch_size\n    pad_w = (patch_size - original_w % patch_size) % patch_size\n    \n    padded_image = cv2.copyMakeBorder(\n        image, \n        0, pad_h, \n        0, pad_w, \n        cv2.BORDER_CONSTANT, \n        value=[0, 0, 0]\n    )\n    \n    # Convertir en float32 et normaliser\n    padded_image = padded_image.astype(np.float32) / 255.0\n    padded_image = np.transpose(padded_image, (2, 0, 1))  # (C, H, W)\n    \n    # Initialiser le masque complet\n    full_mask = np.zeros((padded_image.shape[1], padded_image.shape[2]), dtype=np.float32)\n    \n    # D√©coupage en patches et pr√©diction\n    model.eval()\n    with torch.no_grad():\n        for y in range(0, padded_image.shape[1], patch_size):\n            for x in range(0, padded_image.shape[2], patch_size):\n                # Extraire le patch\n                patch = padded_image[\n                    :, \n                    y:y+patch_size, \n                    x:x+patch_size\n                ]\n                \n                # Convertir en tensor et pr√©dire\n                patch_tensor = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).to(device)\n                output = model(patch_tensor)\n                \n                # Post-traitement\n                prob = torch.sigmoid(output)\n                pred = (prob > 0.5).float().cpu().numpy().squeeze()\n                \n                # Ajouter au masque complet\n                full_mask[y:y+patch_size, x:x+patch_size] = pred\n                \n    # Retirer le padding et retourner le masque final\n    final_mask = full_mask[:original_h, :original_w]\n    return final_mask\n\n# %% [code] Cellule 11 : Exemple d'utilisation sur une image test\n# Charger le mod√®le sauvegard√©\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,  # D√©sactiver les poids pr√©-entra√Æn√©s\n    in_channels=3,\n    classes=1\n)\nmodel.load_state_dict(torch.load('road_segmentation_model.pth', map_location=device))\nmodel.to(device)\n\n# Chemin vers une image test (√† adapter selon votre structure)\ntest_image_path = \"/kaggle/input/testcity/region_25_sat.png\"\n\n# G√©n√©rer la pr√©diction\npredicted_mask = predict_large_image(test_image_path, model)\n\n# Sauvegarder le r√©sultat\ncv2.imwrite(\"prediction_finale.png\", (predicted_mask * 255).astype(np.uint8))\nprint(\"Pr√©diction sauvegard√©e sous 'prediction_finale.png'\")\n\n# Visualisation (optionnelle)\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.imshow(cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB))\nplt.title('Image originale')\nplt.axis('off')\n\nplt.subplot(122)\nplt.imshow(predicted_mask, cmap='gray')\nplt.title('Masque pr√©dit')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nimport segmentation_models_pytorch as smp\n\n# V√©rifier si un GPU est disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Utilisation de l'appareil :\", device)\n\n# =================== Classe du Dataset ===================\nclass Sentinel2Dataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None, patch_size=512, stride=512):\n        self.transform = transform\n        self.patch_size = patch_size\n        self.stride = stride\n        \n        self.image_dir = os.path.join(root_dir, split, \"images\")\n        self.mask_dir = os.path.join(root_dir, split, \"masks\")\n        \n        self.image_files = sorted(glob.glob(os.path.join(self.image_dir, \"*\")))\n        self.mask_files = sorted(glob.glob(os.path.join(self.mask_dir, \"*\")))\n        \n        assert len(self.image_files) == len(self.mask_files), \"Mismatch images/masks!\"\n        \n        self.patches = []\n        for img_path, mask_path in zip(self.image_files, self.mask_files):\n            img = cv2.imread(img_path)\n            if img is None:\n                continue\n            h, w, _ = img.shape\n            for y in range(0, h, self.stride):\n                for x in range(0, w, self.stride):\n                    if y + self.patch_size <= h and x + self.patch_size <= w:\n                        self.patches.append((img_path, mask_path, x, y))\n                        \n        print(f\"Nombre de patchs pour {split}: {len(self.patches)}\")\n    \n    def __len__(self):\n        return len(self.patches)\n    \n    def __getitem__(self, index):\n        img_path, mask_path, x, y = self.patches[index]\n\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    \n        image_patch = image[y:y+self.patch_size, x:x+self.patch_size]\n        mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]\n\n        mask_patch = mask_patch.astype(np.float32) / 255.0\n    \n        if self.transform:\n           augmented = self.transform(image=image_patch, mask=mask_patch)\n           image_patch = augmented['image']\n           mask_patch = augmented['mask']\n\n        image_patch = image_patch.astype(np.float32) / 255.0\n        image_patch = np.transpose(image_patch, (2, 0, 1))\n        image_patch = torch.tensor(image_patch, dtype=torch.float)\n    \n        mask_patch = torch.tensor(mask_patch, dtype=torch.float).unsqueeze(0)\n    \n        return image_patch, mask_patch\n\n# =================== Transformations ===================\ndef transformer(p=0.5):\n    return A.Compose([\n        A.RandomResizedCrop(height=512, width=512, p=0.2),\n        A.ShiftScaleRotate(p=0.5, shift_limit=0.0625, scale_limit=0.05, rotate_limit=10),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        A.RandomRotate90(p=0.5),\n    ], p=p)\n\n# =================== Chargement du dataset ===================\ndataset_root = \"/kaggle/input/deepglobe/deepglobe\"\n\ntrain_dataset = Sentinel2Dataset(root_dir=dataset_root, split='train', transform=transformer(p=0.5))\nvalid_dataset = Sentinel2Dataset(root_dir=dataset_root, split='valid', transform=None)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\n\n# =================== Chargement du mod√®le pr√©-entra√Æn√© ===================\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,  # Pas besoin d'ImageNet, on charge les poids entra√Æn√©s\n    in_channels=3,\n    classes=1,\n)\n\n# Charger les poids du mod√®le entra√Æn√©\nmodel.load_state_dict(torch.load(\"/kaggle/input/modelupp/pytorch/default/1/road_segmentation_model.pth\", map_location=device))\n\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n# Envoyer sur GPU/CPU\nmodel.to(device)\n\n# =================== D√©finition de la loss et de l'optimiseur ===================\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# =================== Fonction de calcul des m√©triques ===================\ndef compute_metrics(outputs, masks, threshold=0.5):\n    probs = torch.sigmoid(outputs)\n    preds = (probs > threshold).float()\n    \n    masks = masks.float()\n    TP = (preds * masks).sum()\n    FP = (preds * (1 - masks)).sum()\n    FN = ((1 - preds) * masks).sum()\n    \n    return TP.item(), FP.item(), FN.item()\n\ndef calculate_epoch_metrics(TP, FP, FN, eps=1e-6):\n    IoU = TP / (TP + FP + FN + eps)\n    precision = TP / (TP + FP + eps)\n    recall = TP / (TP + FN + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    return IoU, precision, recall, f1\n\n# =================== R√©entra√Ænement du mod√®le ===================\nnum_epochs = 6  # Nombre d'√©poques suppl√©mentaires\nmetrics_history = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    train_TP, train_FP, train_FN = 0, 0, 0\n    \n    for images, masks in train_loader:\n        images, masks = images.to(device), masks.to(device).float()\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n        TP, FP, FN = compute_metrics(outputs, masks)\n        train_TP += TP\n        train_FP += FP\n        train_FN += FN\n        \n    train_loss /= len(train_loader.dataset)\n    train_IoU, train_precision, train_recall, train_f1 = calculate_epoch_metrics(train_TP, train_FP, train_FN)\n    \n    model.eval()\n    valid_loss = 0.0\n    valid_TP, valid_FP, valid_FN = 0, 0, 0\n    with torch.no_grad():\n        for images, masks in valid_loader:\n            images, masks = images.to(device), masks.to(device).float()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            valid_loss += loss.item() * images.size(0)\n            TP, FP, FN = compute_metrics(outputs, masks)\n            valid_TP += TP\n            valid_FP += FP\n            valid_FN += FN\n            \n    valid_loss /= len(valid_loader.dataset)\n    valid_IoU, valid_precision, valid_recall, valid_f1 = calculate_epoch_metrics(valid_TP, valid_FP, valid_FN)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {valid_loss:.4f}\")\n    \n    metrics_history.append({\n         'epoch': epoch+1,\n         'train_loss': train_loss,\n         'valid_loss': valid_loss,\n         'train_IoU': train_IoU,\n         'train_precision': train_precision,\n         'train_recall': train_recall,\n         'train_f1': train_f1,\n         'valid_IoU': valid_IoU,\n         'valid_precision': valid_precision,\n         'valid_recall': valid_recall,\n         'valid_f1': valid_f1,\n    })\n\n# Sauvegarde du mod√®le mis √† jour\ntorch.save({'model_state_dict': model.state_dict()}, \"road_segmentation_updated.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:38:39.750926Z","iopub.execute_input":"2025-03-06T07:38:39.751282Z","iopub.status.idle":"2025-03-06T07:39:40.694865Z","shell.execute_reply.started":"2025-03-06T07:38:39.751254Z","shell.execute_reply":"2025-03-06T07:39:40.693760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/input/modelupp/pytorch/default/1/road_segmentation_model.pth\", map_location=device)\n\nif \"model_state_dict\" in checkpoint:\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    print(\"‚úÖ Mod√®le charg√© avec 'model_state_dict'.\")\nelif \"state_dict\" in checkpoint:\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    print(\"‚úÖ Mod√®le charg√© avec 'state_dict'.\")\nelif isinstance(checkpoint, dict):\n    model.load_state_dict(checkpoint)\n    print(\"‚úÖ Mod√®le charg√© directement.\")\nelse:\n    model = checkpoint  # Chargement direct si le mod√®le entier a √©t√© sauvegard√©\n    print(\"‚úÖ Mod√®le charg√© compl√®tement.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:41:53.947732Z","iopub.execute_input":"2025-03-06T07:41:53.948101Z","iopub.status.idle":"2025-03-06T07:41:54.207896Z","shell.execute_reply.started":"2025-03-06T07:41:53.948070Z","shell.execute_reply":"2025-03-06T07:41:54.207035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nimport segmentation_models_pytorch as smp\n\n# V√©rifier si un GPU est disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"‚úÖ Utilisation de l'appareil :\", device)\n\n# =================== Classe du Dataset ===================\nclass Sentinel2Dataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None, patch_size=512, stride=512):\n        self.transform = transform\n        self.patch_size = patch_size\n        self.stride = stride\n        \n        self.image_dir = os.path.join(root_dir, split, \"images\")\n        self.mask_dir = os.path.join(root_dir, split, \"masks\")\n        \n        self.image_files = sorted(glob.glob(os.path.join(self.image_dir, \"*\")))\n        self.mask_files = sorted(glob.glob(os.path.join(self.mask_dir, \"*\")))\n        \n        assert len(self.image_files) == len(self.mask_files), \"Mismatch images/masks!\"\n        \n        self.patches = []\n        for img_path, mask_path in zip(self.image_files, self.mask_files):\n            img = cv2.imread(img_path)\n            if img is None:\n                continue\n            h, w, _ = img.shape\n            for y in range(0, h, self.stride):\n                for x in range(0, w, self.stride):\n                    if y + self.patch_size <= h and x + self.patch_size <= w:\n                        self.patches.append((img_path, mask_path, x, y))\n                        \n        print(f\"üìå Nombre de patchs pour {split}: {len(self.patches)}\")\n    \n    def __len__(self):\n        return len(self.patches)\n    \n    def __getitem__(self, index):\n        img_path, mask_path, x, y = self.patches[index]\n\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    \n        image_patch = image[y:y+self.patch_size, x:x+self.patch_size]\n        mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]\n\n        mask_patch = mask_patch.astype(np.float32) / 255.0\n    \n        if self.transform:\n           augmented = self.transform(image=image_patch, mask=mask_patch)\n           image_patch = augmented['image']\n           mask_patch = augmented['mask']\n\n        image_patch = image_patch.astype(np.float32) / 255.0\n        image_patch = np.transpose(image_patch, (2, 0, 1))\n        image_patch = torch.tensor(image_patch, dtype=torch.float)\n    \n        mask_patch = torch.tensor(mask_patch, dtype=torch.float).unsqueeze(0)\n    \n        return image_patch, mask_patch\n\n# =================== Transformations ===================\ndef transformer(p=0.5):\n    return A.Compose([\n        A.RandomResizedCrop(height=512, width=512, p=0.2),\n        A.ShiftScaleRotate(p=0.5, shift_limit=0.0625, scale_limit=0.05, rotate_limit=10),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        A.RandomRotate90(p=0.5),\n    ], p=p)\n\n# =================== Chargement du dataset ===================\ndataset_root = \"/kaggle/input/deepglobe/deepglobe\"\n\ntrain_dataset = Sentinel2Dataset(root_dir=dataset_root, split='train', transform=transformer(p=0.5))\nvalid_dataset = Sentinel2Dataset(root_dir=dataset_root, split='valid', transform=None)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\n\n# =================== Chargement du mod√®le pr√©-entra√Æn√© ===================\nmodel_path = \"/kaggle/input/modelupp/pytorch/default/1/road_segmentation_model.pth\"\n\ncheckpoint = torch.load(model_path, map_location=device)\n\n# V√©rifier comment le mod√®le a √©t√© sauvegard√©\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,\n    in_channels=3,\n    classes=1\n)\n\n# Charger les poids directement sans passer par un dictionnaire\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.to(device)\nmodel.eval()\n\nprint(\"‚úÖ Mod√®le charg√© avec succ√®s avec load_state_dict() !\")\n\n\n# =================== Fonction de test sur des images ===================\ndef test_model(dataloader, num_samples=3):\n    model.eval()\n    with torch.no_grad():\n        for i, (images, masks) in enumerate(dataloader):\n            if i >= num_samples:\n                break\n            images, masks = images.to(device), masks.to(device)\n            outputs = torch.sigmoid(model(images))  # Appliquer sigmoid\n            preds = (outputs > 0.5).float()\n            \n            img = np.transpose(images[0].cpu().numpy(), (1, 2, 0))\n            mask_gt = masks[0].cpu().numpy()[0]\n            mask_pred = preds[0].cpu().numpy()[0]\n            \n            plt.figure(figsize=(12, 4))\n            plt.subplot(1, 3, 1)\n            plt.imshow(img)\n            plt.title(\"Image Originale\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 2)\n            plt.imshow(mask_gt, cmap='gray')\n            plt.title(\"Masque V√©rit√© Terrain\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 3)\n            plt.imshow(mask_pred, cmap='gray')\n            plt.title(\"Masque Pr√©dit\")\n            plt.axis(\"off\")\n\n            plt.show()\n\n# Test du mod√®le sur quelques images de validation\ntest_model(valid_loader)\n\n# =================== Fine-Tuning (R√©entra√Ænement) ===================\nnum_epochs = 5  # Nombre d'√©poques pour le fine-tuning\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-5)  # R√©duction du learning rate\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for images, masks in train_loader:\n        images, masks = images.to(device), masks.to(device)\n        optimizer.zero_grad()\n        \n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    print(f\"üìå Epoch {epoch+1}/{num_epochs} - Loss: {total_loss / len(train_loader):.4f}\")\n\n# =================== Sauvegarde du mod√®le mis √† jour ===================\ntorch.save(model, \"/kaggle/working\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:49:28.808069Z","iopub.execute_input":"2025-03-06T07:49:28.808415Z","iopub.status.idle":"2025-03-06T11:32:39.109589Z","shell.execute_reply.started":"2025-03-06T07:49:28.808388Z","shell.execute_reply":"2025-03-06T11:32:39.108240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sauvegarde du mod√®le sous un fichier .pth\ntorch.save(model.state_dict(), \"/kaggle/working/unetpp_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:34:02.632632Z","iopub.execute_input":"2025-03-06T11:34:02.632948Z","iopub.status.idle":"2025-03-06T11:34:02.937204Z","shell.execute_reply.started":"2025-03-06T11:34:02.632923Z","shell.execute_reply":"2025-03-06T11:34:02.936551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_large_image(image_path, model, patch_size=512, device=\"cuda\"):\n    # Charger l'image\n    image = cv2.imread(image_path)\n\n    # V√©rifier si l'image est charg√©e correctement\n    if image is None:\n        raise FileNotFoundError(f\"Erreur : Impossible de charger l'image '{image_path}'. V√©rifiez le chemin du fichier.\")\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original_h, original_w, _ = image.shape\n    \n    # Appliquer un padding si n√©cessaire\n    pad_h = (patch_size - original_h % patch_size) % patch_size\n    pad_w = (patch_size - original_w % patch_size) % patch_size\n    \n    padded_image = cv2.copyMakeBorder(\n        image, \n        0, pad_h, \n        0, pad_w, \n        cv2.BORDER_CONSTANT, \n        value=[0, 0, 0]\n    )\n    \n    # Convertir en float32 et normaliser\n    padded_image = padded_image.astype(np.float32) / 255.0\n    padded_image = np.transpose(padded_image, (2, 0, 1))  # (C, H, W)\n    \n    # Initialiser le masque complet\n    full_mask = np.zeros((padded_image.shape[1], padded_image.shape[2]), dtype=np.float32)\n    \n    # D√©coupage en patches et pr√©diction\n    model.eval()\n    with torch.no_grad():\n        for y in range(0, padded_image.shape[1], patch_size):\n            for x in range(0, padded_image.shape[2], patch_size):\n                # Extraire le patch\n                patch = padded_image[\n                    :, \n                    y:y+patch_size, \n                    x:x+patch_size\n                ]\n                \n                # Convertir en tensor et pr√©dire\n                patch_tensor = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).to(device)\n                output = model(patch_tensor)\n                \n                # Post-traitement\n                prob = torch.sigmoid(output)\n                pred = (prob > 0.5).float().cpu().numpy().squeeze()\n                \n                # Ajouter au masque complet\n                full_mask[y:y+patch_size, x:x+patch_size] = pred\n                \n    # Retirer le padding et retourner le masque final\n    final_mask = full_mask[:original_h, :original_w]\n    return final_mask\n\n# %% [code] Cellule 11 : Exemple d'utilisation sur une image test\n# Charger le mod√®le sauvegard√©\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,  # D√©sactiver les poids pr√©-entra√Æn√©s\n    in_channels=3,\n    classes=1\n)\nmodel.load_state_dict(torch.load('unetpp_model.pth', map_location=device))\nmodel.to(device)\n\n# Chemin vers une image test (√† adapter selon votre structure)\ntest_image_path = \"/kaggle/input/deepglobe/deepglobe/valid/images/100712_sat.jpg\"\n\n# G√©n√©rer la pr√©diction\npredicted_mask = predict_large_image(test_image_path, model)\n\n# Sauvegarder le r√©sultat\ncv2.imwrite(\"prediction_finale.jpg\", (predicted_mask * 255).astype(np.uint8))\nprint(\"Pr√©diction sauvegard√©e sous 'prediction_finale.jpg'\")\n\n# Visualisation (optionnelle)\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.imshow(cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB))\nplt.title('Image originale')\nplt.axis('off')\n\nplt.subplot(122)\nplt.imshow(predicted_mask, cmap='gray')\nplt.title('Masque pr√©dit')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:39:14.282845Z","iopub.execute_input":"2025-03-06T11:39:14.283197Z","iopub.status.idle":"2025-03-06T11:39:15.872595Z","shell.execute_reply.started":"2025-03-06T11:39:14.283170Z","shell.execute_reply":"2025-03-06T11:39:15.871572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n\ndef evaluate_model(model, dataloader, device=\"cuda\"):\n    model.eval()\n    total_iou, total_precision, total_recall, total_f1 = 0, 0, 0, 0\n    num_samples = 0\n    \n    with torch.no_grad():\n        for images, masks in dataloader:\n            batch_size, num_patches, channels, height, width = images.shape\n            images = images.view(batch_size * num_patches, channels, height, width).to(device)\n            masks = masks.view(batch_size * num_patches, 1, height, width).to(device)\n\n            outputs = model(images)\n            preds = (outputs > 0.5).float()  # Binarisation des pr√©dictions\n            \n            # Conversion en vecteur pour sklearn\n            preds = preds.cpu().numpy().flatten()\n            masks = masks.cpu().numpy().flatten()\n\n            total_iou += jaccard_score(masks, preds)\n            total_precision += precision_score(masks, preds)\n            total_recall += recall_score(masks, preds)\n            total_f1 += f1_score(masks, preds)\n\n            num_samples += 1\n    \n    # Moyenne des m√©triques sur tous les √©chantillons\n    avg_iou = total_iou / num_samples\n    avg_precision = total_precision / num_samples\n    avg_recall = total_recall / num_samples\n    avg_f1 = total_f1 / num_samples\n\n    return avg_iou, avg_precision, avg_recall, avg_f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:39:45.747482Z","iopub.execute_input":"2025-03-06T11:39:45.747791Z","iopub.status.idle":"2025-03-06T11:39:46.127904Z","shell.execute_reply.started":"2025-03-06T11:39:45.747768Z","shell.execute_reply":"2025-03-06T11:39:46.127227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âvaluation du mod√®le apr√®s l'entra√Ænement\niou, precision, recall, f1 = evaluate_model(model, valid_loader, device)\n\nprint(\"\\nüîπ **R√©sultats apr√®s entra√Ænement :**\")\nprint(f\"IoU (Intersection over Union)  : {iou:.4f}\")\nprint(f\"Pr√©cision                      : {precision:.4f}\")\nprint(f\"Rappel                         : {recall:.4f}\")\nprint(f\"F1-score                       : {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:42:00.438690Z","iopub.execute_input":"2025-03-06T11:42:00.439060Z","iopub.status.idle":"2025-03-06T11:42:01.368073Z","shell.execute_reply.started":"2025-03-06T11:42:00.439028Z","shell.execute_reply":"2025-03-06T11:42:01.366831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n\ndef evaluate_model(model, dataloader, device=\"cuda\"):\n    model.eval()\n    iou_scores, precision_scores, recall_scores, f1_scores = [], [], [], []\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"√âvaluation en cours\"):\n            images, masks = images.to(device), masks.to(device)\n\n            # Pr√©dictions\n            outputs = model(images)\n            preds = (outputs > 0.5).float()  # Binarisation des pr√©dictions\n\n            # Conversion en numpy sans .flatten()\n            preds_np = preds.cpu().numpy().astype(int)\n            masks_np = masks.cpu().numpy().astype(int)\n\n            # Calcul des m√©triques sur l'ensemble du batch\n            for i in range(preds_np.shape[0]):  # Parcourir chaque image du batch\n                iou_scores.append(jaccard_score(masks_np[i].flatten(), preds_np[i].flatten()))\n                precision_scores.append(precision_score(masks_np[i].flatten(), preds_np[i].flatten()))\n                recall_scores.append(recall_score(masks_np[i].flatten(), preds_np[i].flatten()))\n                f1_scores.append(f1_score(masks_np[i].flatten(), preds_np[i].flatten()))\n\n    # Moyenne des scores\n    avg_iou = np.mean(iou_scores)\n    avg_precision = np.mean(precision_scores)\n    avg_recall = np.mean(recall_scores)\n    avg_f1 = np.mean(f1_scores)\n\n    return avg_iou, avg_precision, avg_recall, avg_f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:57:29.625766Z","iopub.execute_input":"2025-03-06T11:57:29.626097Z","iopub.status.idle":"2025-03-06T11:57:29.633809Z","shell.execute_reply.started":"2025-03-06T11:57:29.626069Z","shell.execute_reply":"2025-03-06T11:57:29.632903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chargement de toutes les images de validation\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T12:06:44.275087Z","iopub.execute_input":"2025-03-06T12:06:44.275402Z","iopub.status.idle":"2025-03-06T12:06:44.279311Z","shell.execute_reply.started":"2025-03-06T12:06:44.275379Z","shell.execute_reply":"2025-03-06T12:06:44.278455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# V√©rification du device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# √âvaluer le mod√®le optimis√©\niou, precision, recall, f1 = evaluate_model(model, valid_loader, device)\n\n# Affichage des r√©sultats\nprint(\"\\nüîπ **R√©sultats apr√®s entra√Ænement :**\")\nprint(f\"IoU (Intersection over Union)  : {iou:.4f}\")\nprint(f\"Pr√©cision                      : {precision:.4f}\")\nprint(f\"Rappel                         : {recall:.4f}\")\nprint(f\"F1-score                       : {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T12:06:49.349352Z","iopub.execute_input":"2025-03-06T12:06:49.349640Z","iopub.status.idle":"2025-03-06T12:53:11.669104Z","shell.execute_reply.started":"2025-03-06T12:06:49.349618Z","shell.execute_reply":"2025-03-06T12:53:11.668019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Stocker les m√©triques\nmetrics = [iou, precision, recall, f1]\nlabels = [\"IoU\", \"Pr√©cision\", \"Rappel\", \"F1-score\"]\n\n# Affichage des m√©triques\nplt.figure(figsize=(8, 5))\nplt.bar(labels, metrics, color=[\"blue\", \"green\", \"orange\", \"red\"])\nplt.ylim(0, 1)\nplt.ylabel(\"Score\")\nplt.title(\"M√©triques de Performance du Mod√®le U-Net++\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T12:53:26.053527Z","iopub.execute_input":"2025-03-06T12:53:26.053848Z","iopub.status.idle":"2025-03-06T12:53:26.193330Z","shell.execute_reply.started":"2025-03-06T12:53:26.053819Z","shell.execute_reply":"2025-03-06T12:53:26.192536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_large_image(image_path, model, patch_size=512, device=\"cuda\"):\n    # Charger l'image\n    image = cv2.imread(image_path)\n\n    # V√©rifier si l'image est charg√©e correctement\n    if image is None:\n        raise FileNotFoundError(f\"Erreur : Impossible de charger l'image '{image_path}'. V√©rifiez le chemin du fichier.\")\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original_h, original_w, _ = image.shape\n    \n    # Appliquer un padding si n√©cessaire\n    pad_h = (patch_size - original_h % patch_size) % patch_size\n    pad_w = (patch_size - original_w % patch_size) % patch_size\n    \n    padded_image = cv2.copyMakeBorder(\n        image, \n        0, pad_h, \n        0, pad_w, \n        cv2.BORDER_CONSTANT, \n        value=[0, 0, 0]\n    )\n    \n    # Convertir en float32 et normaliser\n    padded_image = padded_image.astype(np.float32) / 255.0\n    padded_image = np.transpose(padded_image, (2, 0, 1))  # (C, H, W)\n    \n    # Initialiser le masque complet\n    full_mask = np.zeros((padded_image.shape[1], padded_image.shape[2]), dtype=np.float32)\n    \n    # D√©coupage en patches et pr√©diction\n    model.eval()\n    with torch.no_grad():\n        for y in range(0, padded_image.shape[1], patch_size):\n            for x in range(0, padded_image.shape[2], patch_size):\n                # Extraire le patch\n                patch = padded_image[\n                    :, \n                    y:y+patch_size, \n                    x:x+patch_size\n                ]\n                \n                # Convertir en tensor et pr√©dire\n                patch_tensor = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).to(device)\n                output = model(patch_tensor)\n                \n                # Post-traitement\n                prob = torch.sigmoid(output)\n                pred = (prob > 0.5).float().cpu().numpy().squeeze()\n                \n                # Ajouter au masque complet\n                full_mask[y:y+patch_size, x:x+patch_size] = pred\n                \n    # Retirer le padding et retourner le masque final\n    final_mask = full_mask[:original_h, :original_w]\n    return final_mask\n\n# %% [code] Cellule 11 : Exemple d'utilisation sur une image test\n# Charger le mod√®le sauvegard√©\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,  # D√©sactiver les poids pr√©-entra√Æn√©s\n    in_channels=3,\n    classes=1\n)\nmodel.load_state_dict(torch.load('unetpp_model.pth', map_location=device))\nmodel.to(device)\n\n# Chemin vers une image test (√† adapter selon votre structure)\ntest_image_path = \"/kaggle/input/testcity/region_25_sat.png\"\n\n# G√©n√©rer la pr√©diction\npredicted_mask = predict_large_image(test_image_path, model)\n\n# Sauvegarder le r√©sultat\ncv2.imwrite(\"prediction_finale.png\", (predicted_mask * 255).astype(np.uint8))\nprint(\"Pr√©diction sauvegard√©e sous 'prediction_finale.png'\")\n\n# Visualisation (optionnelle)\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.imshow(cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB))\nplt.title('Image originale')\nplt.axis('off')\n\nplt.subplot(122)\nplt.imshow(predicted_mask, cmap='gray')\nplt.title('Masque pr√©dit')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:59:52.054644Z","iopub.execute_input":"2025-03-06T11:59:52.054944Z","iopub.status.idle":"2025-03-06T11:59:55.206124Z","shell.execute_reply.started":"2025-03-06T11:59:52.054921Z","shell.execute_reply":"2025-03-06T11:59:55.205192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_large_image(image_path, model, patch_size=512, device=\"cuda\"):\n    # Charger l'image\n    image = cv2.imread(image_path)\n\n    # V√©rifier si l'image est charg√©e correctement\n    if image is None:\n        raise FileNotFoundError(f\"Erreur : Impossible de charger l'image '{image_path}'. V√©rifiez le chemin du fichier.\")\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original_h, original_w, _ = image.shape\n    \n    # Appliquer un padding si n√©cessaire\n    pad_h = (patch_size - original_h % patch_size) % patch_size\n    pad_w = (patch_size - original_w % patch_size) % patch_size\n    \n    padded_image = cv2.copyMakeBorder(\n        image, \n        0, pad_h, \n        0, pad_w, \n        cv2.BORDER_CONSTANT, \n        value=[0, 0, 0]\n    )\n    \n    # Convertir en float32 et normaliser\n    padded_image = padded_image.astype(np.float32) / 255.0\n    padded_image = np.transpose(padded_image, (2, 0, 1))  # (C, H, W)\n    \n    # Initialiser le masque complet\n    full_mask = np.zeros((padded_image.shape[1], padded_image.shape[2]), dtype=np.float32)\n    \n    # D√©coupage en patches et pr√©diction\n    model.eval()\n    with torch.no_grad():\n        for y in range(0, padded_image.shape[1], patch_size):\n            for x in range(0, padded_image.shape[2], patch_size):\n                # Extraire le patch\n                patch = padded_image[\n                    :, \n                    y:y+patch_size, \n                    x:x+patch_size\n                ]\n                \n                # Convertir en tensor et pr√©dire\n                patch_tensor = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).to(device)\n                output = model(patch_tensor)\n                \n                # Post-traitement\n                prob = torch.sigmoid(output)\n                pred = (prob > 0.5).float().cpu().numpy().squeeze()\n                \n                # Ajouter au masque complet\n                full_mask[y:y+patch_size, x:x+patch_size] = pred\n                \n    # Retirer le padding et retourner le masque final\n    final_mask = full_mask[:original_h, :original_w]\n    return final_mask\n\n# %% [code] Cellule 11 : Exemple d'utilisation sur une image test\n# Charger le mod√®le sauvegard√©\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,  # D√©sactiver les poids pr√©-entra√Æn√©s\n    in_channels=3,\n    classes=1\n)\nmodel.load_state_dict(torch.load('unetpp_model.pth', map_location=device))\nmodel.to(device)\n\n# Chemin vers une image test (√† adapter selon votre structure)\ntest_image_path = \"/kaggle/input/deepglobe/deepglobe/valid/images/114405_sat.jpg\"\n\n# G√©n√©rer la pr√©diction\npredicted_mask = predict_large_image(test_image_path, model)\n\n# Sauvegarder le r√©sultat\ncv2.imwrite(\"prediction_finale.jpg\", (predicted_mask * 255).astype(np.uint8))\nprint(\"Pr√©diction sauvegard√©e sous 'prediction_finale.jpg'\")\n\n# Visualisation (optionnelle)\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.imshow(cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB))\nplt.title('Image originale')\nplt.axis('off')\n\nplt.subplot(122)\nplt.imshow(predicted_mask, cmap='gray')\nplt.title('Masque pr√©dit')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:01:05.150129Z","iopub.execute_input":"2025-03-06T13:01:05.150424Z","iopub.status.idle":"2025-03-06T13:01:06.676002Z","shell.execute_reply.started":"2025-03-06T13:01:05.150402Z","shell.execute_reply":"2025-03-06T13:01:06.675110Z"}},"outputs":[],"execution_count":null}]}