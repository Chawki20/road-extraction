{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10929263,"sourceType":"datasetVersion","datasetId":6795467,"isSourceIdPinned":true},{"sourceId":10934538,"sourceType":"datasetVersion","datasetId":6799526},{"sourceId":276840,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":237070,"modelId":258759}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install segmentation-models-pytorch\n!pip install lightning albumentations","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:27:21.816547Z","iopub.execute_input":"2025-03-06T07:27:21.816853Z","iopub.status.idle":"2025-03-06T07:27:28.832874Z","shell.execute_reply.started":"2025-03-06T07:27:21.816827Z","shell.execute_reply":"2025-03-06T07:27:28.831823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 1 : Import des bibliothèques nécessaires\nimport os\nimport glob\nimport json\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\n# Vous pouvez ajouter d'autres imports d'albumentations si besoin\n\nimport segmentation_models_pytorch as smp\n\n# Vérifier si un GPU est disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Utilisation de l'appareil :\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:07:30.146762Z","iopub.execute_input":"2025-03-06T07:07:30.147102Z","iopub.status.idle":"2025-03-06T07:07:40.975832Z","shell.execute_reply.started":"2025-03-06T07:07:30.147075Z","shell.execute_reply":"2025-03-06T07:07:40.975100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] : Classe dataset adaptée sans fichier JSON\nclass Sentinel2Dataset(Dataset):\n    \"\"\"\n    Dataset pour la segmentation des routes.\n    Il extrait des patchs de taille patch_size x patch_size à partir d'images de grande taille\n    en utilisant une fenêtre glissante sans chevauchement.\n    \"\"\"\n    def __init__(self, root_dir, split='train', transform=None, patch_size=512, stride=512):\n        \"\"\"\n        Args:\n            root_dir (str): Chemin racine du dataset (ex: \"/kaggle/input/mon_dataset\").\n            split (str): 'train', 'valid' ou 'test'.\n            transform: Transformations à appliquer (albumentations).\n            patch_size (int): Taille du patch (par défaut 512).\n            stride (int): Pas de la fenêtre glissante (par défaut 512, donc sans chevauchement).\n        \"\"\"\n        self.transform = transform\n        self.patch_size = patch_size\n        self.stride = stride\n        \n        # Définir les répertoires d'images et de masques en fonction du split\n        self.image_dir = os.path.join(root_dir, split, \"images\")\n        self.mask_dir = os.path.join(root_dir, split, \"masks\")\n        \n        # Récupérer la liste des fichiers d'images et masques\n        self.image_files = sorted(glob.glob(os.path.join(self.image_dir, \"*\")))\n        self.mask_files = sorted(glob.glob(os.path.join(self.mask_dir, \"*\")))\n        \n        # Vérifier que le nombre d'images et de masques correspond\n        assert len(self.image_files) == len(self.mask_files), \"Le nombre d'images et de masques ne correspond pas!\"\n        \n        # Créer la liste des patchs : pour chaque image, on stocke (chemin_image, chemin_masque, x, y)\n        self.patches = []\n        for img_path, mask_path in zip(self.image_files, self.mask_files):\n            img = cv2.imread(img_path)\n            if img is None:\n                continue\n            h, w, _ = img.shape\n            for y in range(0, h, self.stride):\n                for x in range(0, w, self.stride):\n                    if y + self.patch_size <= h and x + self.patch_size <= w:\n                        self.patches.append((img_path, mask_path, x, y))\n                        \n        print(f\"Nombre de patchs pour le split {split} : {len(self.patches)}\")\n    \n    def __len__(self):\n        return len(self.patches)\n    \n    def __getitem__(self, index):\n        img_path, mask_path, x, y = self.patches[index]\n        # Lire l'image et le masque\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    \n        # Extraire le patch correspondant\n        image_patch = image[y:y+self.patch_size, x:x+self.patch_size]\n        mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]\n    \n        # Normaliser le masque (convertir 0/255 en 0/1)\n        mask_patch = mask_patch.astype(np.float32) / 255.0\n    \n        # Appliquer les transformations (si définies, par exemple pour l'entraînement)\n        if self.transform:\n           augmented = self.transform(image=image_patch, mask=mask_patch)\n           image_patch = augmented['image']\n           mask_patch = augmented['mask']\n    \n        # Normaliser l'image de [0, 255] à [0, 1]\n        image_patch = image_patch.astype(np.float32) / 255.0\n       # Changer l'ordre des canaux pour obtenir (C, H, W)\n        image_patch = np.transpose(image_patch, (2, 0, 1))\n        image_patch = torch.tensor(image_patch, dtype=torch.float)\n    \n        # Convertir le masque en tensor (la dimension du canal sera ajoutée pour correspondre à la sortie du modèle)\n        mask_patch = torch.tensor(mask_patch, dtype=torch.float)\n        mask_patch = mask_patch.unsqueeze(0)\n    \n        return image_patch, mask_patch\n\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:08:08.505728Z","iopub.execute_input":"2025-03-06T07:08:08.506074Z","iopub.status.idle":"2025-03-06T07:08:08.516862Z","shell.execute_reply.started":"2025-03-06T07:08:08.506049Z","shell.execute_reply":"2025-03-06T07:08:08.515996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 3 : Définition de la fonction d'augmentation\ndef transformer(p=0.5):\n    \"\"\"\n    Fonction de transformation utilisant albumentations pour la data augmentation.\n    Seules les images d'entraînement seront augmentées.\n    \"\"\"\n    return A.Compose([\n        # Recadrage aléatoire redimensionné\n        A.RandomResizedCrop(\n            height=512,\n            width=512,\n            p=0.2\n        ),\n        # Déplacement, mise à l'échelle et rotation\n        A.ShiftScaleRotate(\n            p=0.5,\n            shift_limit=0.0625,\n            scale_limit=0.05,\n            rotate_limit=10,\n            border_mode=0,  # bord constant\n            value=0,\n            mask_value=0,\n            interpolation=2,  # interpolation bicubique\n        ),\n        # Flip, transposition et rotation de 90 degrés aléatoire\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        A.RandomRotate90(p=0.5),\n    ], p=p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:08:14.216524Z","iopub.execute_input":"2025-03-06T07:08:14.216810Z","iopub.status.idle":"2025-03-06T07:08:14.222221Z","shell.execute_reply.started":"2025-03-06T07:08:14.216789Z","shell.execute_reply":"2025-03-06T07:08:14.220958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] : Création des datasets et DataLoaders sans fichier JSON\n# Définir le chemin racine de votre dataset Kaggle\ndataset_root = \"/kaggle/input/deepglobe/deepglobe\"  # Adaptez ce chemin selon votre environnement\n\n# Création des datasets pour l'entraînement et la validation\ntrain_dataset = Sentinel2Dataset(root_dir=dataset_root, split='train', transform=transformer(p=0.5))\nvalid_dataset = Sentinel2Dataset(root_dir=dataset_root, split='valid', transform=None)\n\n# Création des DataLoaders (batch = 8)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:08:19.695031Z","iopub.execute_input":"2025-03-06T07:08:19.695341Z","iopub.status.idle":"2025-03-06T07:10:57.602242Z","shell.execute_reply.started":"2025-03-06T07:08:19.695316Z","shell.execute_reply":"2025-03-06T07:10:57.601377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 5 : Définition du modèle U-Net avec ResNet50\n# Création du modèle avec segmentation_models_pytorch\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",        # Choix du backbone\n    encoder_weights=\"imagenet\",       # Poids pré-entraînés sur ImageNet\n    in_channels=3,                    # Entrée en RGB\n    classes=1,                        # Segmentation binaire (1 canal en sortie)\n)\n\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:35:20.925947Z","iopub.execute_input":"2025-03-05T22:35:20.926274Z","iopub.status.idle":"2025-03-05T22:35:22.539906Z","shell.execute_reply.started":"2025-03-05T22:35:20.926249Z","shell.execute_reply":"2025-03-05T22:35:22.539072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 6 : Définition de la fonction de perte et de l'optimiseur\ncriterion = nn.BCEWithLogitsLoss()  # Adapté pour la segmentation binaire\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:35:30.766138Z","iopub.execute_input":"2025-03-05T22:35:30.766425Z","iopub.status.idle":"2025-03-05T22:35:30.771444Z","shell.execute_reply.started":"2025-03-05T22:35:30.766405Z","shell.execute_reply":"2025-03-05T22:35:30.770613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 7 : Fonctions de calcul des métriques\ndef compute_metrics(outputs, masks, threshold=0.5):\n    \"\"\"\n    Calcule les vrais positifs (TP), faux positifs (FP) et faux négatifs (FN) pour un batch.\n    \n    Args:\n        outputs (tensor): sorties brutes du modèle (logits) de taille (B, 1, H, W)\n        masks (tensor): masques ground truth de taille (B, 1, H, W)\n        threshold (float): seuil pour binariser la sortie (après sigmoid)\n        \n    Returns:\n        TP, FP, FN cumulés pour le batch.\n    \"\"\"\n    # Appliquer la sigmoid pour obtenir les probabilités\n    probs = torch.sigmoid(outputs)\n    preds = (probs > threshold).float()\n    \n    masks = masks.float()\n    TP = (preds * masks).sum()\n    FP = (preds * (1 - masks)).sum()\n    FN = ((1 - preds) * masks).sum()\n    \n    return TP.item(), FP.item(), FN.item()\n\ndef calculate_epoch_metrics(TP, FP, FN, eps=1e-6):\n    \"\"\"\n    Calcule IoU, Précision, Rappel et F1-score à partir des sommes de TP, FP et FN.\n    \"\"\"\n    IoU = TP / (TP + FP + FN + eps)\n    precision = TP / (TP + FP + eps)\n    recall = TP / (TP + FN + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    return IoU, precision, recall, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:35:33.987826Z","iopub.execute_input":"2025-03-05T22:35:33.988138Z","iopub.status.idle":"2025-03-05T22:35:33.993596Z","shell.execute_reply.started":"2025-03-05T22:35:33.988113Z","shell.execute_reply":"2025-03-05T22:35:33.992734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 8 : Boucle d'entraînement et validation avec enregistrement des métriques\nnum_epochs = 8  # Vous pouvez ajuster le nombre d'époques\nmetrics_history = []   # Liste pour stocker les métriques par époque\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    train_TP, train_FP, train_FN = 0, 0, 0\n    \n    for images, masks in train_loader:\n        images = images.to(device)\n        masks = masks.to(device).float()  # Conversion en float pour la loss\n        \n        optimizer.zero_grad()\n        outputs = model(images)  # Sortie de taille (B, 1, H, W)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n        TP, FP, FN = compute_metrics(outputs, masks)\n        train_TP += TP\n        train_FP += FP\n        train_FN += FN\n        \n    train_loss /= len(train_loader.dataset)\n    train_IoU, train_precision, train_recall, train_f1 = calculate_epoch_metrics(train_TP, train_FP, train_FN)\n    \n    # Phase de validation\n    model.eval()\n    valid_loss = 0.0\n    valid_TP, valid_FP, valid_FN = 0, 0, 0\n    with torch.no_grad():\n        for images, masks in valid_loader:\n            images = images.to(device)\n            masks = masks.to(device).float()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            valid_loss += loss.item() * images.size(0)\n            TP, FP, FN = compute_metrics(outputs, masks)\n            valid_TP += TP\n            valid_FP += FP\n            valid_FN += FN\n            \n    valid_loss /= len(valid_loader.dataset)\n    valid_IoU, valid_precision, valid_recall, valid_f1 = calculate_epoch_metrics(valid_TP, valid_FP, valid_FN)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {valid_loss:.4f}\")\n    print(f\"Train - IoU: {train_IoU:.4f}, Précision: {train_precision:.4f}, Rappel: {train_recall:.4f}, F1: {train_f1:.4f}\")\n    print(f\"Valid - IoU: {valid_IoU:.4f}, Précision: {valid_precision:.4f}, Rappel: {valid_recall:.4f}, F1: {valid_f1:.4f}\\n\")\n    \n    # Enregistrer les métriques de l'époque\n    metrics_history.append({\n         'epoch': epoch+1,\n         'train_loss': train_loss,\n         'valid_loss': valid_loss,\n         'train_IoU': train_IoU,\n         'train_precision': train_precision,\n         'train_recall': train_recall,\n         'train_f1': train_f1,\n         'valid_IoU': valid_IoU,\n         'valid_precision': valid_precision,\n         'valid_recall': valid_recall,\n         'valid_f1': valid_f1,\n    })\n\n# Sauvegarder l'historique des métriques dans un fichier CSV\nmetrics_df = pd.DataFrame(metrics_history)\nmetrics_df.to_csv(\"metrics_history111.csv\", index=False)\nprint(\"Les métriques ont été sauvegardées dans 'metrics_history111.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:35:37.772166Z","iopub.execute_input":"2025-03-05T22:35:37.772458Z","iopub.status.idle":"2025-03-06T04:59:08.130056Z","shell.execute_reply.started":"2025-03-05T22:35:37.772437Z","shell.execute_reply":"2025-03-06T04:59:08.129196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 9 : Affichage des courbes d'évolution des métriques\nepochs = metrics_df['epoch']\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(epochs, metrics_df['train_IoU'], label='Train IoU')\nplt.plot(epochs, metrics_df['valid_IoU'], label='Valid IoU')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"IoU\")\nplt.legend()\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs, metrics_df['train_precision'], label='Train Précision')\nplt.plot(epochs, metrics_df['valid_precision'], label='Valid Précision')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Précision\")\nplt.legend()\n\nplt.subplot(2, 2, 3)\nplt.plot(epochs, metrics_df['train_recall'], label='Train Rappel')\nplt.plot(epochs, metrics_df['valid_recall'], label='Valid Rappel')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Rappel\")\nplt.legend()\n\nplt.subplot(2, 2, 4)\nplt.plot(epochs, metrics_df['train_f1'], label='Train F1-score')\nplt.plot(epochs, metrics_df['valid_f1'], label='Valid F1-score')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"F1-score\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T05:53:27.566118Z","iopub.execute_input":"2025-03-06T05:53:27.566398Z","iopub.status.idle":"2025-03-06T05:53:27.639957Z","shell.execute_reply.started":"2025-03-06T05:53:27.566375Z","shell.execute_reply":"2025-03-06T05:53:27.638904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] Cellule 9 : Sauvegarde du modèle entraîné\n# Sauvegarder le modèle après l'entraînement\ntorch.save(model.state_dict(), 'road_segmentation_model.pth')\nprint(\"Modèle sauvegardé avec succès.\")\n\n# %% [code] Cellule 10 : Fonction d'inférence pour les grandes images\ndef predict_large_image(image_path, model, patch_size=512, device=device):\n    \"\"\"\n    Prédit le masque de segmentation pour une grande image sans redimensionnement.\n    Découpe l'image en patches, effectue la prédiction sur chaque patch et recompose le masque final.\n    \"\"\"\n    # Charger l'image\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original_h, original_w, _ = image.shape\n    \n    # Appliquer un padding si nécessaire\n    pad_h = (patch_size - original_h % patch_size) % patch_size\n    pad_w = (patch_size - original_w % patch_size) % patch_size\n    \n    padded_image = cv2.copyMakeBorder(\n        image, \n        0, pad_h, \n        0, pad_w, \n        cv2.BORDER_CONSTANT, \n        value=[0, 0, 0]\n    )\n    \n    # Convertir en float32 et normaliser\n    padded_image = padded_image.astype(np.float32) / 255.0\n    padded_image = np.transpose(padded_image, (2, 0, 1))  # (C, H, W)\n    \n    # Initialiser le masque complet\n    full_mask = np.zeros((padded_image.shape[1], padded_image.shape[2]), dtype=np.float32)\n    \n    # Découpage en patches et prédiction\n    model.eval()\n    with torch.no_grad():\n        for y in range(0, padded_image.shape[1], patch_size):\n            for x in range(0, padded_image.shape[2], patch_size):\n                # Extraire le patch\n                patch = padded_image[\n                    :, \n                    y:y+patch_size, \n                    x:x+patch_size\n                ]\n                \n                # Convertir en tensor et prédire\n                patch_tensor = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).to(device)\n                output = model(patch_tensor)\n                \n                # Post-traitement\n                prob = torch.sigmoid(output)\n                pred = (prob > 0.5).float().cpu().numpy().squeeze()\n                \n                # Ajouter au masque complet\n                full_mask[y:y+patch_size, x:x+patch_size] = pred\n                \n    # Retirer le padding et retourner le masque final\n    final_mask = full_mask[:original_h, :original_w]\n    return final_mask\n\n# %% [code] Cellule 11 : Exemple d'utilisation sur une image test\n# Charger le modèle sauvegardé\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,  # Désactiver les poids pré-entraînés\n    in_channels=3,\n    classes=1\n)\nmodel.load_state_dict(torch.load('road_segmentation_model.pth', map_location=device))\nmodel.to(device)\n\n# Chemin vers une image test (à adapter selon votre structure)\ntest_image_path = \"/kaggle/input/testcity/region_25_sat.png\"\n\n# Générer la prédiction\npredicted_mask = predict_large_image(test_image_path, model)\n\n# Sauvegarder le résultat\ncv2.imwrite(\"prediction_finale.png\", (predicted_mask * 255).astype(np.uint8))\nprint(\"Prédiction sauvegardée sous 'prediction_finale.png'\")\n\n# Visualisation (optionnelle)\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.imshow(cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB))\nplt.title('Image originale')\nplt.axis('off')\n\nplt.subplot(122)\nplt.imshow(predicted_mask, cmap='gray')\nplt.title('Masque prédit')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nimport segmentation_models_pytorch as smp\n\n# Vérifier si un GPU est disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Utilisation de l'appareil :\", device)\n\n# =================== Classe du Dataset ===================\nclass Sentinel2Dataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None, patch_size=512, stride=512):\n        self.transform = transform\n        self.patch_size = patch_size\n        self.stride = stride\n        \n        self.image_dir = os.path.join(root_dir, split, \"images\")\n        self.mask_dir = os.path.join(root_dir, split, \"masks\")\n        \n        self.image_files = sorted(glob.glob(os.path.join(self.image_dir, \"*\")))\n        self.mask_files = sorted(glob.glob(os.path.join(self.mask_dir, \"*\")))\n        \n        assert len(self.image_files) == len(self.mask_files), \"Mismatch images/masks!\"\n        \n        self.patches = []\n        for img_path, mask_path in zip(self.image_files, self.mask_files):\n            img = cv2.imread(img_path)\n            if img is None:\n                continue\n            h, w, _ = img.shape\n            for y in range(0, h, self.stride):\n                for x in range(0, w, self.stride):\n                    if y + self.patch_size <= h and x + self.patch_size <= w:\n                        self.patches.append((img_path, mask_path, x, y))\n                        \n        print(f\"Nombre de patchs pour {split}: {len(self.patches)}\")\n    \n    def __len__(self):\n        return len(self.patches)\n    \n    def __getitem__(self, index):\n        img_path, mask_path, x, y = self.patches[index]\n\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    \n        image_patch = image[y:y+self.patch_size, x:x+self.patch_size]\n        mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]\n\n        mask_patch = mask_patch.astype(np.float32) / 255.0\n    \n        if self.transform:\n           augmented = self.transform(image=image_patch, mask=mask_patch)\n           image_patch = augmented['image']\n           mask_patch = augmented['mask']\n\n        image_patch = image_patch.astype(np.float32) / 255.0\n        image_patch = np.transpose(image_patch, (2, 0, 1))\n        image_patch = torch.tensor(image_patch, dtype=torch.float)\n    \n        mask_patch = torch.tensor(mask_patch, dtype=torch.float).unsqueeze(0)\n    \n        return image_patch, mask_patch\n\n# =================== Transformations ===================\ndef transformer(p=0.5):\n    return A.Compose([\n        A.RandomResizedCrop(height=512, width=512, p=0.2),\n        A.ShiftScaleRotate(p=0.5, shift_limit=0.0625, scale_limit=0.05, rotate_limit=10),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        A.RandomRotate90(p=0.5),\n    ], p=p)\n\n# =================== Chargement du dataset ===================\ndataset_root = \"/kaggle/input/deepglobe/deepglobe\"\n\ntrain_dataset = Sentinel2Dataset(root_dir=dataset_root, split='train', transform=transformer(p=0.5))\nvalid_dataset = Sentinel2Dataset(root_dir=dataset_root, split='valid', transform=None)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\n\n# =================== Chargement du modèle pré-entraîné ===================\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,  # Pas besoin d'ImageNet, on charge les poids entraînés\n    in_channels=3,\n    classes=1,\n)\n\n# Charger les poids du modèle entraîné\nmodel.load_state_dict(torch.load(\"/kaggle/input/modelupp/pytorch/default/1/road_segmentation_model.pth\", map_location=device))\n\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n# Envoyer sur GPU/CPU\nmodel.to(device)\n\n# =================== Définition de la loss et de l'optimiseur ===================\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# =================== Fonction de calcul des métriques ===================\ndef compute_metrics(outputs, masks, threshold=0.5):\n    probs = torch.sigmoid(outputs)\n    preds = (probs > threshold).float()\n    \n    masks = masks.float()\n    TP = (preds * masks).sum()\n    FP = (preds * (1 - masks)).sum()\n    FN = ((1 - preds) * masks).sum()\n    \n    return TP.item(), FP.item(), FN.item()\n\ndef calculate_epoch_metrics(TP, FP, FN, eps=1e-6):\n    IoU = TP / (TP + FP + FN + eps)\n    precision = TP / (TP + FP + eps)\n    recall = TP / (TP + FN + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    return IoU, precision, recall, f1\n\n# =================== Réentraînement du modèle ===================\nnum_epochs = 6  # Nombre d'époques supplémentaires\nmetrics_history = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    train_TP, train_FP, train_FN = 0, 0, 0\n    \n    for images, masks in train_loader:\n        images, masks = images.to(device), masks.to(device).float()\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n        TP, FP, FN = compute_metrics(outputs, masks)\n        train_TP += TP\n        train_FP += FP\n        train_FN += FN\n        \n    train_loss /= len(train_loader.dataset)\n    train_IoU, train_precision, train_recall, train_f1 = calculate_epoch_metrics(train_TP, train_FP, train_FN)\n    \n    model.eval()\n    valid_loss = 0.0\n    valid_TP, valid_FP, valid_FN = 0, 0, 0\n    with torch.no_grad():\n        for images, masks in valid_loader:\n            images, masks = images.to(device), masks.to(device).float()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            valid_loss += loss.item() * images.size(0)\n            TP, FP, FN = compute_metrics(outputs, masks)\n            valid_TP += TP\n            valid_FP += FP\n            valid_FN += FN\n            \n    valid_loss /= len(valid_loader.dataset)\n    valid_IoU, valid_precision, valid_recall, valid_f1 = calculate_epoch_metrics(valid_TP, valid_FP, valid_FN)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {valid_loss:.4f}\")\n    \n    metrics_history.append({\n         'epoch': epoch+1,\n         'train_loss': train_loss,\n         'valid_loss': valid_loss,\n         'train_IoU': train_IoU,\n         'train_precision': train_precision,\n         'train_recall': train_recall,\n         'train_f1': train_f1,\n         'valid_IoU': valid_IoU,\n         'valid_precision': valid_precision,\n         'valid_recall': valid_recall,\n         'valid_f1': valid_f1,\n    })\n\n# Sauvegarde du modèle mis à jour\ntorch.save({'model_state_dict': model.state_dict()}, \"road_segmentation_updated.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:38:39.750926Z","iopub.execute_input":"2025-03-06T07:38:39.751282Z","iopub.status.idle":"2025-03-06T07:39:40.694865Z","shell.execute_reply.started":"2025-03-06T07:38:39.751254Z","shell.execute_reply":"2025-03-06T07:39:40.693760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/input/modelupp/pytorch/default/1/road_segmentation_model.pth\", map_location=device)\n\nif \"model_state_dict\" in checkpoint:\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    print(\"✅ Modèle chargé avec 'model_state_dict'.\")\nelif \"state_dict\" in checkpoint:\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    print(\"✅ Modèle chargé avec 'state_dict'.\")\nelif isinstance(checkpoint, dict):\n    model.load_state_dict(checkpoint)\n    print(\"✅ Modèle chargé directement.\")\nelse:\n    model = checkpoint  # Chargement direct si le modèle entier a été sauvegardé\n    print(\"✅ Modèle chargé complètement.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:41:53.947732Z","iopub.execute_input":"2025-03-06T07:41:53.948101Z","iopub.status.idle":"2025-03-06T07:41:54.207896Z","shell.execute_reply.started":"2025-03-06T07:41:53.948070Z","shell.execute_reply":"2025-03-06T07:41:54.207035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nimport segmentation_models_pytorch as smp\n\n# Vérifier si un GPU est disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"✅ Utilisation de l'appareil :\", device)\n\n# =================== Classe du Dataset ===================\nclass Sentinel2Dataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None, patch_size=512, stride=512):\n        self.transform = transform\n        self.patch_size = patch_size\n        self.stride = stride\n        \n        self.image_dir = os.path.join(root_dir, split, \"images\")\n        self.mask_dir = os.path.join(root_dir, split, \"masks\")\n        \n        self.image_files = sorted(glob.glob(os.path.join(self.image_dir, \"*\")))\n        self.mask_files = sorted(glob.glob(os.path.join(self.mask_dir, \"*\")))\n        \n        assert len(self.image_files) == len(self.mask_files), \"Mismatch images/masks!\"\n        \n        self.patches = []\n        for img_path, mask_path in zip(self.image_files, self.mask_files):\n            img = cv2.imread(img_path)\n            if img is None:\n                continue\n            h, w, _ = img.shape\n            for y in range(0, h, self.stride):\n                for x in range(0, w, self.stride):\n                    if y + self.patch_size <= h and x + self.patch_size <= w:\n                        self.patches.append((img_path, mask_path, x, y))\n                        \n        print(f\"📌 Nombre de patchs pour {split}: {len(self.patches)}\")\n    \n    def __len__(self):\n        return len(self.patches)\n    \n    def __getitem__(self, index):\n        img_path, mask_path, x, y = self.patches[index]\n\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    \n        image_patch = image[y:y+self.patch_size, x:x+self.patch_size]\n        mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]\n\n        mask_patch = mask_patch.astype(np.float32) / 255.0\n    \n        if self.transform:\n           augmented = self.transform(image=image_patch, mask=mask_patch)\n           image_patch = augmented['image']\n           mask_patch = augmented['mask']\n\n        image_patch = image_patch.astype(np.float32) / 255.0\n        image_patch = np.transpose(image_patch, (2, 0, 1))\n        image_patch = torch.tensor(image_patch, dtype=torch.float)\n    \n        mask_patch = torch.tensor(mask_patch, dtype=torch.float).unsqueeze(0)\n    \n        return image_patch, mask_patch\n\n# =================== Transformations ===================\ndef transformer(p=0.5):\n    return A.Compose([\n        A.RandomResizedCrop(height=512, width=512, p=0.2),\n        A.ShiftScaleRotate(p=0.5, shift_limit=0.0625, scale_limit=0.05, rotate_limit=10),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        A.RandomRotate90(p=0.5),\n    ], p=p)\n\n# =================== Chargement du dataset ===================\ndataset_root = \"/kaggle/input/deepglobe/deepglobe\"\n\ntrain_dataset = Sentinel2Dataset(root_dir=dataset_root, split='train', transform=transformer(p=0.5))\nvalid_dataset = Sentinel2Dataset(root_dir=dataset_root, split='valid', transform=None)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\n\n# =================== Chargement du modèle pré-entraîné ===================\nmodel_path = \"/kaggle/input/modelupp/pytorch/default/1/road_segmentation_model.pth\"\n\ncheckpoint = torch.load(model_path, map_location=device)\n\n# Vérifier comment le modèle a été sauvegardé\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,\n    in_channels=3,\n    classes=1\n)\n\n# Charger les poids directement sans passer par un dictionnaire\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.to(device)\nmodel.eval()\n\nprint(\"✅ Modèle chargé avec succès avec load_state_dict() !\")\n\n\n# =================== Fonction de test sur des images ===================\ndef test_model(dataloader, num_samples=3):\n    model.eval()\n    with torch.no_grad():\n        for i, (images, masks) in enumerate(dataloader):\n            if i >= num_samples:\n                break\n            images, masks = images.to(device), masks.to(device)\n            outputs = torch.sigmoid(model(images))  # Appliquer sigmoid\n            preds = (outputs > 0.5).float()\n            \n            img = np.transpose(images[0].cpu().numpy(), (1, 2, 0))\n            mask_gt = masks[0].cpu().numpy()[0]\n            mask_pred = preds[0].cpu().numpy()[0]\n            \n            plt.figure(figsize=(12, 4))\n            plt.subplot(1, 3, 1)\n            plt.imshow(img)\n            plt.title(\"Image Originale\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 2)\n            plt.imshow(mask_gt, cmap='gray')\n            plt.title(\"Masque Vérité Terrain\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 3)\n            plt.imshow(mask_pred, cmap='gray')\n            plt.title(\"Masque Prédit\")\n            plt.axis(\"off\")\n\n            plt.show()\n\n# Test du modèle sur quelques images de validation\ntest_model(valid_loader)\n\n# =================== Fine-Tuning (Réentraînement) ===================\nnum_epochs = 5  # Nombre d'époques pour le fine-tuning\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-5)  # Réduction du learning rate\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for images, masks in train_loader:\n        images, masks = images.to(device), masks.to(device)\n        optimizer.zero_grad()\n        \n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    print(f\"📌 Epoch {epoch+1}/{num_epochs} - Loss: {total_loss / len(train_loader):.4f}\")\n\n# =================== Sauvegarde du modèle mis à jour ===================\ntorch.save(model, \"/kaggle/working\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T07:49:28.808069Z","iopub.execute_input":"2025-03-06T07:49:28.808415Z","iopub.status.idle":"2025-03-06T11:32:39.109589Z","shell.execute_reply.started":"2025-03-06T07:49:28.808388Z","shell.execute_reply":"2025-03-06T11:32:39.108240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sauvegarde du modèle sous un fichier .pth\ntorch.save(model.state_dict(), \"/kaggle/working/unetpp_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:34:02.632632Z","iopub.execute_input":"2025-03-06T11:34:02.632948Z","iopub.status.idle":"2025-03-06T11:34:02.937204Z","shell.execute_reply.started":"2025-03-06T11:34:02.632923Z","shell.execute_reply":"2025-03-06T11:34:02.936551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_large_image(image_path, model, patch_size=512, device=\"cuda\"):\n    # Charger l'image\n    image = cv2.imread(image_path)\n\n    # Vérifier si l'image est chargée correctement\n    if image is None:\n        raise FileNotFoundError(f\"Erreur : Impossible de charger l'image '{image_path}'. Vérifiez le chemin du fichier.\")\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original_h, original_w, _ = image.shape\n    \n    # Appliquer un padding si nécessaire\n    pad_h = (patch_size - original_h % patch_size) % patch_size\n    pad_w = (patch_size - original_w % patch_size) % patch_size\n    \n    padded_image = cv2.copyMakeBorder(\n        image, \n        0, pad_h, \n        0, pad_w, \n        cv2.BORDER_CONSTANT, \n        value=[0, 0, 0]\n    )\n    \n    # Convertir en float32 et normaliser\n    padded_image = padded_image.astype(np.float32) / 255.0\n    padded_image = np.transpose(padded_image, (2, 0, 1))  # (C, H, W)\n    \n    # Initialiser le masque complet\n    full_mask = np.zeros((padded_image.shape[1], padded_image.shape[2]), dtype=np.float32)\n    \n    # Découpage en patches et prédiction\n    model.eval()\n    with torch.no_grad():\n        for y in range(0, padded_image.shape[1], patch_size):\n            for x in range(0, padded_image.shape[2], patch_size):\n                # Extraire le patch\n                patch = padded_image[\n                    :, \n                    y:y+patch_size, \n                    x:x+patch_size\n                ]\n                \n                # Convertir en tensor et prédire\n                patch_tensor = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).to(device)\n                output = model(patch_tensor)\n                \n                # Post-traitement\n                prob = torch.sigmoid(output)\n                pred = (prob > 0.5).float().cpu().numpy().squeeze()\n                \n                # Ajouter au masque complet\n                full_mask[y:y+patch_size, x:x+patch_size] = pred\n                \n    # Retirer le padding et retourner le masque final\n    final_mask = full_mask[:original_h, :original_w]\n    return final_mask\n\n# %% [code] Cellule 11 : Exemple d'utilisation sur une image test\n# Charger le modèle sauvegardé\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,  # Désactiver les poids pré-entraînés\n    in_channels=3,\n    classes=1\n)\nmodel.load_state_dict(torch.load('unetpp_model.pth', map_location=device))\nmodel.to(device)\n\n# Chemin vers une image test (à adapter selon votre structure)\ntest_image_path = \"/kaggle/input/deepglobe/deepglobe/valid/images/100712_sat.jpg\"\n\n# Générer la prédiction\npredicted_mask = predict_large_image(test_image_path, model)\n\n# Sauvegarder le résultat\ncv2.imwrite(\"prediction_finale.jpg\", (predicted_mask * 255).astype(np.uint8))\nprint(\"Prédiction sauvegardée sous 'prediction_finale.jpg'\")\n\n# Visualisation (optionnelle)\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.imshow(cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB))\nplt.title('Image originale')\nplt.axis('off')\n\nplt.subplot(122)\nplt.imshow(predicted_mask, cmap='gray')\nplt.title('Masque prédit')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:39:14.282845Z","iopub.execute_input":"2025-03-06T11:39:14.283197Z","iopub.status.idle":"2025-03-06T11:39:15.872595Z","shell.execute_reply.started":"2025-03-06T11:39:14.283170Z","shell.execute_reply":"2025-03-06T11:39:15.871572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n\ndef evaluate_model(model, dataloader, device=\"cuda\"):\n    model.eval()\n    total_iou, total_precision, total_recall, total_f1 = 0, 0, 0, 0\n    num_samples = 0\n    \n    with torch.no_grad():\n        for images, masks in dataloader:\n            batch_size, num_patches, channels, height, width = images.shape\n            images = images.view(batch_size * num_patches, channels, height, width).to(device)\n            masks = masks.view(batch_size * num_patches, 1, height, width).to(device)\n\n            outputs = model(images)\n            preds = (outputs > 0.5).float()  # Binarisation des prédictions\n            \n            # Conversion en vecteur pour sklearn\n            preds = preds.cpu().numpy().flatten()\n            masks = masks.cpu().numpy().flatten()\n\n            total_iou += jaccard_score(masks, preds)\n            total_precision += precision_score(masks, preds)\n            total_recall += recall_score(masks, preds)\n            total_f1 += f1_score(masks, preds)\n\n            num_samples += 1\n    \n    # Moyenne des métriques sur tous les échantillons\n    avg_iou = total_iou / num_samples\n    avg_precision = total_precision / num_samples\n    avg_recall = total_recall / num_samples\n    avg_f1 = total_f1 / num_samples\n\n    return avg_iou, avg_precision, avg_recall, avg_f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:39:45.747482Z","iopub.execute_input":"2025-03-06T11:39:45.747791Z","iopub.status.idle":"2025-03-06T11:39:46.127904Z","shell.execute_reply.started":"2025-03-06T11:39:45.747768Z","shell.execute_reply":"2025-03-06T11:39:46.127227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Évaluation du modèle après l'entraînement\niou, precision, recall, f1 = evaluate_model(model, valid_loader, device)\n\nprint(\"\\n🔹 **Résultats après entraînement :**\")\nprint(f\"IoU (Intersection over Union)  : {iou:.4f}\")\nprint(f\"Précision                      : {precision:.4f}\")\nprint(f\"Rappel                         : {recall:.4f}\")\nprint(f\"F1-score                       : {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:42:00.438690Z","iopub.execute_input":"2025-03-06T11:42:00.439060Z","iopub.status.idle":"2025-03-06T11:42:01.368073Z","shell.execute_reply.started":"2025-03-06T11:42:00.439028Z","shell.execute_reply":"2025-03-06T11:42:01.366831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n\ndef evaluate_model(model, dataloader, device=\"cuda\"):\n    model.eval()\n    iou_scores, precision_scores, recall_scores, f1_scores = [], [], [], []\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"Évaluation en cours\"):\n            images, masks = images.to(device), masks.to(device)\n\n            # Prédictions\n            outputs = model(images)\n            preds = (outputs > 0.5).float()  # Binarisation des prédictions\n\n            # Conversion en numpy sans .flatten()\n            preds_np = preds.cpu().numpy().astype(int)\n            masks_np = masks.cpu().numpy().astype(int)\n\n            # Calcul des métriques sur l'ensemble du batch\n            for i in range(preds_np.shape[0]):  # Parcourir chaque image du batch\n                iou_scores.append(jaccard_score(masks_np[i].flatten(), preds_np[i].flatten()))\n                precision_scores.append(precision_score(masks_np[i].flatten(), preds_np[i].flatten()))\n                recall_scores.append(recall_score(masks_np[i].flatten(), preds_np[i].flatten()))\n                f1_scores.append(f1_score(masks_np[i].flatten(), preds_np[i].flatten()))\n\n    # Moyenne des scores\n    avg_iou = np.mean(iou_scores)\n    avg_precision = np.mean(precision_scores)\n    avg_recall = np.mean(recall_scores)\n    avg_f1 = np.mean(f1_scores)\n\n    return avg_iou, avg_precision, avg_recall, avg_f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:57:29.625766Z","iopub.execute_input":"2025-03-06T11:57:29.626097Z","iopub.status.idle":"2025-03-06T11:57:29.633809Z","shell.execute_reply.started":"2025-03-06T11:57:29.626069Z","shell.execute_reply":"2025-03-06T11:57:29.632903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chargement de toutes les images de validation\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T12:06:44.275087Z","iopub.execute_input":"2025-03-06T12:06:44.275402Z","iopub.status.idle":"2025-03-06T12:06:44.279311Z","shell.execute_reply.started":"2025-03-06T12:06:44.275379Z","shell.execute_reply":"2025-03-06T12:06:44.278455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vérification du device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Évaluer le modèle optimisé\niou, precision, recall, f1 = evaluate_model(model, valid_loader, device)\n\n# Affichage des résultats\nprint(\"\\n🔹 **Résultats après entraînement :**\")\nprint(f\"IoU (Intersection over Union)  : {iou:.4f}\")\nprint(f\"Précision                      : {precision:.4f}\")\nprint(f\"Rappel                         : {recall:.4f}\")\nprint(f\"F1-score                       : {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T12:06:49.349352Z","iopub.execute_input":"2025-03-06T12:06:49.349640Z","iopub.status.idle":"2025-03-06T12:53:11.669104Z","shell.execute_reply.started":"2025-03-06T12:06:49.349618Z","shell.execute_reply":"2025-03-06T12:53:11.668019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Stocker les métriques\nmetrics = [iou, precision, recall, f1]\nlabels = [\"IoU\", \"Précision\", \"Rappel\", \"F1-score\"]\n\n# Affichage des métriques\nplt.figure(figsize=(8, 5))\nplt.bar(labels, metrics, color=[\"blue\", \"green\", \"orange\", \"red\"])\nplt.ylim(0, 1)\nplt.ylabel(\"Score\")\nplt.title(\"Métriques de Performance du Modèle U-Net++\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T12:53:26.053527Z","iopub.execute_input":"2025-03-06T12:53:26.053848Z","iopub.status.idle":"2025-03-06T12:53:26.193330Z","shell.execute_reply.started":"2025-03-06T12:53:26.053819Z","shell.execute_reply":"2025-03-06T12:53:26.192536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_large_image(image_path, model, patch_size=512, device=\"cuda\"):\n    # Charger l'image\n    image = cv2.imread(image_path)\n\n    # Vérifier si l'image est chargée correctement\n    if image is None:\n        raise FileNotFoundError(f\"Erreur : Impossible de charger l'image '{image_path}'. Vérifiez le chemin du fichier.\")\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original_h, original_w, _ = image.shape\n    \n    # Appliquer un padding si nécessaire\n    pad_h = (patch_size - original_h % patch_size) % patch_size\n    pad_w = (patch_size - original_w % patch_size) % patch_size\n    \n    padded_image = cv2.copyMakeBorder(\n        image, \n        0, pad_h, \n        0, pad_w, \n        cv2.BORDER_CONSTANT, \n        value=[0, 0, 0]\n    )\n    \n    # Convertir en float32 et normaliser\n    padded_image = padded_image.astype(np.float32) / 255.0\n    padded_image = np.transpose(padded_image, (2, 0, 1))  # (C, H, W)\n    \n    # Initialiser le masque complet\n    full_mask = np.zeros((padded_image.shape[1], padded_image.shape[2]), dtype=np.float32)\n    \n    # Découpage en patches et prédiction\n    model.eval()\n    with torch.no_grad():\n        for y in range(0, padded_image.shape[1], patch_size):\n            for x in range(0, padded_image.shape[2], patch_size):\n                # Extraire le patch\n                patch = padded_image[\n                    :, \n                    y:y+patch_size, \n                    x:x+patch_size\n                ]\n                \n                # Convertir en tensor et prédire\n                patch_tensor = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).to(device)\n                output = model(patch_tensor)\n                \n                # Post-traitement\n                prob = torch.sigmoid(output)\n                pred = (prob > 0.5).float().cpu().numpy().squeeze()\n                \n                # Ajouter au masque complet\n                full_mask[y:y+patch_size, x:x+patch_size] = pred\n                \n    # Retirer le padding et retourner le masque final\n    final_mask = full_mask[:original_h, :original_w]\n    return final_mask\n\n# %% [code] Cellule 11 : Exemple d'utilisation sur une image test\n# Charger le modèle sauvegardé\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,  # Désactiver les poids pré-entraînés\n    in_channels=3,\n    classes=1\n)\nmodel.load_state_dict(torch.load('unetpp_model.pth', map_location=device))\nmodel.to(device)\n\n# Chemin vers une image test (à adapter selon votre structure)\ntest_image_path = \"/kaggle/input/testcity/region_25_sat.png\"\n\n# Générer la prédiction\npredicted_mask = predict_large_image(test_image_path, model)\n\n# Sauvegarder le résultat\ncv2.imwrite(\"prediction_finale.png\", (predicted_mask * 255).astype(np.uint8))\nprint(\"Prédiction sauvegardée sous 'prediction_finale.png'\")\n\n# Visualisation (optionnelle)\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.imshow(cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB))\nplt.title('Image originale')\nplt.axis('off')\n\nplt.subplot(122)\nplt.imshow(predicted_mask, cmap='gray')\nplt.title('Masque prédit')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:59:52.054644Z","iopub.execute_input":"2025-03-06T11:59:52.054944Z","iopub.status.idle":"2025-03-06T11:59:55.206124Z","shell.execute_reply.started":"2025-03-06T11:59:52.054921Z","shell.execute_reply":"2025-03-06T11:59:55.205192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_large_image(image_path, model, patch_size=512, device=\"cuda\"):\n    # Charger l'image\n    image = cv2.imread(image_path)\n\n    # Vérifier si l'image est chargée correctement\n    if image is None:\n        raise FileNotFoundError(f\"Erreur : Impossible de charger l'image '{image_path}'. Vérifiez le chemin du fichier.\")\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original_h, original_w, _ = image.shape\n    \n    # Appliquer un padding si nécessaire\n    pad_h = (patch_size - original_h % patch_size) % patch_size\n    pad_w = (patch_size - original_w % patch_size) % patch_size\n    \n    padded_image = cv2.copyMakeBorder(\n        image, \n        0, pad_h, \n        0, pad_w, \n        cv2.BORDER_CONSTANT, \n        value=[0, 0, 0]\n    )\n    \n    # Convertir en float32 et normaliser\n    padded_image = padded_image.astype(np.float32) / 255.0\n    padded_image = np.transpose(padded_image, (2, 0, 1))  # (C, H, W)\n    \n    # Initialiser le masque complet\n    full_mask = np.zeros((padded_image.shape[1], padded_image.shape[2]), dtype=np.float32)\n    \n    # Découpage en patches et prédiction\n    model.eval()\n    with torch.no_grad():\n        for y in range(0, padded_image.shape[1], patch_size):\n            for x in range(0, padded_image.shape[2], patch_size):\n                # Extraire le patch\n                patch = padded_image[\n                    :, \n                    y:y+patch_size, \n                    x:x+patch_size\n                ]\n                \n                # Convertir en tensor et prédire\n                patch_tensor = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).to(device)\n                output = model(patch_tensor)\n                \n                # Post-traitement\n                prob = torch.sigmoid(output)\n                pred = (prob > 0.5).float().cpu().numpy().squeeze()\n                \n                # Ajouter au masque complet\n                full_mask[y:y+patch_size, x:x+patch_size] = pred\n                \n    # Retirer le padding et retourner le masque final\n    final_mask = full_mask[:original_h, :original_w]\n    return final_mask\n\n# %% [code] Cellule 11 : Exemple d'utilisation sur une image test\n# Charger le modèle sauvegardé\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet50\",\n    encoder_weights=None,  # Désactiver les poids pré-entraînés\n    in_channels=3,\n    classes=1\n)\nmodel.load_state_dict(torch.load('unetpp_model.pth', map_location=device))\nmodel.to(device)\n\n# Chemin vers une image test (à adapter selon votre structure)\ntest_image_path = \"/kaggle/input/deepglobe/deepglobe/valid/images/114405_sat.jpg\"\n\n# Générer la prédiction\npredicted_mask = predict_large_image(test_image_path, model)\n\n# Sauvegarder le résultat\ncv2.imwrite(\"prediction_finale.jpg\", (predicted_mask * 255).astype(np.uint8))\nprint(\"Prédiction sauvegardée sous 'prediction_finale.jpg'\")\n\n# Visualisation (optionnelle)\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.imshow(cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB))\nplt.title('Image originale')\nplt.axis('off')\n\nplt.subplot(122)\nplt.imshow(predicted_mask, cmap='gray')\nplt.title('Masque prédit')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:01:05.150129Z","iopub.execute_input":"2025-03-06T13:01:05.150424Z","iopub.status.idle":"2025-03-06T13:01:06.676002Z","shell.execute_reply.started":"2025-03-06T13:01:05.150402Z","shell.execute_reply":"2025-03-06T13:01:06.675110Z"}},"outputs":[],"execution_count":null}]}